{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reload changes -> always run this\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 10 \n",
    "\n",
    "Ron Cordell, Ted Dunmire, Filip Krunic \n",
    "\n",
    "-----\n",
    "\n",
    "##### Setup \n",
    "\n",
    "To have IPython correctly use PySpark, we import it as a module using the `findspark` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Question 10.1\n",
    "\n",
    "*In Spark write the code to count how often each word appears in a text document (or set of documents). Please use this homework document as a the example document to run an experiment.  Report the following: provide a sorted list of tokens in decreasing order of frequency of occurence.*\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "We first load the text file from a local directory. Once it's placed in a Spark context, it goes through two mapping and one reduce phase which collects all of the terms and returns their count. This is accomplished with `flatMap` which generates key-value pairs using `.split()` to get the tokens. The second map phase assigns `1` to the value for each key. Finally, the data is reduced by key and returned with `.collect()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'y)', 1),\n",
       " (u'sameModel', 1),\n",
       " (u'all', 1),\n",
       " (u'weight(X)=', 1),\n",
       " (u'10.2:', 1),\n",
       " (u'What', 4),\n",
       " (u'not', 1),\n",
       " (u'intuitoive', 1),\n",
       " (u'(cluster', 1),\n",
       " (u'consists', 1),\n",
       " (u'-----------------------', 2),\n",
       " (u'based', 1),\n",
       " (u'parameters', 1),\n",
       " (u'error(point):', 1),\n",
       " (u'(Euclidean', 1),\n",
       " (u'Modify', 1),\n",
       " (u'3', 1),\n",
       " (u'Here', 1),\n",
       " (u'languages', 1),\n",
       " (u'snippet', 1),\n",
       " (u'SQRT(X.X)=', 1),\n",
       " (u'2,', 1),\n",
       " (u'weeks', 1),\n",
       " (u'submissions', 1),\n",
       " (u'homework', 2),\n",
       " (u'case', 1),\n",
       " (u'return', 1),\n",
       " (u'runs=10,', 1),\n",
       " (u'KMeans,', 1),\n",
       " (u'load', 1),\n",
       " (u'notebook:', 2),\n",
       " (u'experiment.', 1),\n",
       " (u'words', 3),\n",
       " (u'homeworks', 1),\n",
       " (u'compute', 1),\n",
       " (u'sc.textFile(\"kmeans_data.txt\")', 1),\n",
       " (u'bringing', 1),\n",
       " (u'Spark', 3),\n",
       " (u'(one', 2),\n",
       " (u'runs', 1),\n",
       " (u'resource', 1),\n",
       " (u'questions===', 1),\n",
       " (u'array([float(x)', 1),\n",
       " (u'evaluation', 2),\n",
       " (u'frequency.', 1),\n",
       " (u'(point', 1),\n",
       " (u'1/||X||,', 1),\n",
       " (u'--', 2),\n",
       " (u'(or', 1),\n",
       " (u'SPECIAL', 1),\n",
       " (u'Learning', 1),\n",
       " (u'KMeansModel.load(sc,', 1),\n",
       " (u'where', 1),\n",
       " (u'iterations', 2),\n",
       " (u'initializationMode=\"random\")', 1),\n",
       " (u'generation', 1),\n",
       " (u'list', 2),\n",
       " (u'Sum', 4),\n",
       " (u'begin', 1),\n",
       " (u'(homegrown', 1),\n",
       " (u'provided).', 1),\n",
       " (u'run', 2),\n",
       " (u'10.4:', 1),\n",
       " (u'Assignments\"', 1),\n",
       " (u'#', 7),\n",
       " (u'sqrt(sum([x**2', 1),\n",
       " (u'for', 11),\n",
       " (u'+', 3),\n",
       " (u'plots.', 2),\n",
       " (u'please', 1),\n",
       " (u'labeled', 1),\n",
       " (u'per', 2),\n",
       " (u'cell', 1),\n",
       " (u'blanks:', 1),\n",
       " (u'above', 1),\n",
       " (u'Report', 2),\n",
       " (u'\"Teams', 1),\n",
       " (u'at:', 1),\n",
       " (u'interfaces', 1),\n",
       " (u'math', 1),\n",
       " (u'available', 3),\n",
       " (u'HW10', 1),\n",
       " (u'lazy', 3),\n",
       " (u'https://www.dropbox.com/s/q85t0ytb9apggnh/kmeans_data.txt?dl=0', 2),\n",
       " (u'LASS0', 2),\n",
       " (u'This', 1),\n",
       " (u'W261', 1),\n",
       " (u'ISVC):', 1),\n",
       " (u'modify', 1),\n",
       " (u'here', 2),\n",
       " (u'20', 1),\n",
       " (u'fun', 1),\n",
       " (u'group', 1),\n",
       " (u'100', 3),\n",
       " (u'Comment', 4),\n",
       " (u'training', 3),\n",
       " (u'Weight', 1),\n",
       " (u'column', 1),\n",
       " (u'completing', 1),\n",
       " (u'count', 3),\n",
       " (u'implementation', 1),\n",
       " (u'03/15/2016', 1),\n",
       " (u'resulting', 1),\n",
       " (u'follows:', 1),\n",
       " (u'UC', 1),\n",
       " (u'comment', 1),\n",
       " (u'letters', 1),\n",
       " (u'?????', 1),\n",
       " (u'==================', 1),\n",
       " (u'distributed', 1),\n",
       " (u'Please', 3),\n",
       " (u'done', 1),\n",
       " (u'10.6:', 1),\n",
       " (u'DATSCI', 1),\n",
       " (u'array', 1),\n",
       " (u'==================END', 1),\n",
       " (u'https://www.dropbox.com/s/3nsthvp8g2rrrdh/EM-Kmeans.ipynb?dl=0', 1),\n",
       " (u'clustering', 1),\n",
       " (u'===', 6),\n",
       " (u'use', 1),\n",
       " (u'from', 5),\n",
       " (u'findings', 1),\n",
       " (u'submit', 1),\n",
       " (u'HW10.5.', 1),\n",
       " (u'(a-z)', 1),\n",
       " (u'error(point)).reduce(lambda', 1),\n",
       " (u'Homeworks', 1),\n",
       " (u'numpy', 1),\n",
       " (u'X2^2)', 1),\n",
       " (u'sort', 1),\n",
       " (u'a', 12),\n",
       " (u'RIDGE', 2),\n",
       " (u'form', 1),\n",
       " (u'=========================', 1),\n",
       " (u'10.6.1', 2),\n",
       " (u'forward', 1),\n",
       " (u'manner.', 1),\n",
       " (u'Apache', 2),\n",
       " (u'(using', 1),\n",
       " (u'with', 7),\n",
       " (u'had', 1),\n",
       " (u'Load', 1),\n",
       " (u'10.0:', 1),\n",
       " (u'Run', 1),\n",
       " (u'word', 2),\n",
       " (u'MLlib-centric', 1),\n",
       " (u'this', 7),\n",
       " (u'V1.3', 1),\n",
       " (u'============================================', 1),\n",
       " (u'See', 1),\n",
       " (u'following', 3),\n",
       " (u\"')]))\", 1),\n",
       " (u'def', 1),\n",
       " (u'and', 23),\n",
       " (u'model?', 1),\n",
       " (u'links', 1),\n",
       " (u'is', 8),\n",
       " (u'Squared', 4),\n",
       " (u'parse', 1),\n",
       " (u'single', 1),\n",
       " (u'Call', 1),\n",
       " (u'tab', 1),\n",
       " (u'KMeans.train(parsedData,', 1),\n",
       " (u'as', 5),\n",
       " (u'WSSSE', 1),\n",
       " (u'different', 1),\n",
       " (u'data)', 1),\n",
       " (u'x,', 1),\n",
       " (u'provide', 1),\n",
       " (u'set.', 2),\n",
       " (u'-', 1),\n",
       " (u'length', 1),\n",
       " (u'sqrt', 1),\n",
       " (u'write', 1),\n",
       " (u'report', 3),\n",
       " (u'HW10.3.', 2),\n",
       " (u'answer', 1),\n",
       " (u'clusters.', 2),\n",
       " (u'program.', 1),\n",
       " (u'=', 8),\n",
       " (u'regression.', 2),\n",
       " (u'Machine', 1),\n",
       " (u'https://docs.google.com/spreadsheets/d/1ncFQl5Tovn-16slD8mYjP_nzMTPSfiGeLLzW8v_sMjg/edit?usp=sharing',\n",
       "  1),\n",
       " (u'===========================================================================',\n",
       "  1),\n",
       " (u'driver', 1),\n",
       " (u'Download', 1),\n",
       " (u'SQRT(X1^2', 1),\n",
       " (u'Berkeley,', 1),\n",
       " (u'develop', 1),\n",
       " (u'data.map(lambda', 1),\n",
       " (u'data', 8),\n",
       " (u'the', 46),\n",
       " (u'tune', 1),\n",
       " (u'documents).', 1),\n",
       " (u'tokens', 1),\n",
       " (u'lower', 1),\n",
       " (u'Mesos', 1),\n",
       " (u'......', 1),\n",
       " (u'point:', 1),\n",
       " (u'NOTE', 2),\n",
       " (u'savings', 1),\n",
       " (u'1', 1),\n",
       " (u'algorithms', 1),\n",
       " (u'pyspark.mllib.clustering', 1),\n",
       " (u'model', 3),\n",
       " (u'hyper', 1),\n",
       " (u'Errors', 3),\n",
       " (u'show', 1),\n",
       " (u'text', 1),\n",
       " (u'Error', 1),\n",
       " (u'results', 3),\n",
       " (u'code', 9),\n",
       " (u'using', 3),\n",
       " (u'==HW', 4),\n",
       " (u'follow', 1),\n",
       " (u'\"Gradient', 1),\n",
       " (u'Final', 1),\n",
       " (u\"line.split('\", 1),\n",
       " (u'inverse', 1),\n",
       " (u'number', 1),\n",
       " (u'Linear', 1),\n",
       " (u'how', 2),\n",
       " (u\"MLlib's\", 1),\n",
       " (u'to', 9),\n",
       " (u'going', 1),\n",
       " (u'team)', 1),\n",
       " (u'Using', 7),\n",
       " (u'10.5:', 1),\n",
       " (u'#10===', 1),\n",
       " (u'X2.', 1),\n",
       " (u'do', 1),\n",
       " (u'good', 1),\n",
       " (u'get', 1),\n",
       " (u'evaluate', 1),\n",
       " (u'Spark,', 1),\n",
       " (u'Kmean', 1),\n",
       " (u'framework', 1),\n",
       " (u'made', 1),\n",
       " (u'carefully.', 1),\n",
       " (u'sorted', 1),\n",
       " (u'kmeans_data.txt', 2),\n",
       " (u'dataset', 1),\n",
       " (u'instructions', 1),\n",
       " (u'In', 2),\n",
       " (u'follows', 1),\n",
       " (u'Within', 3),\n",
       " (u'https://www.dropbox.com/s/atzqkc0p1eajuz6/LinearRegression-Notebook-Challenge.ipynb?dl=0',\n",
       "  1),\n",
       " (u'server', 1),\n",
       " (u'===HW', 3),\n",
       " (u'API', 1),\n",
       " (u'either', 1),\n",
       " (u'each', 4),\n",
       " (u'output', 1),\n",
       " (u'hundred)', 1),\n",
       " (u'Again', 1),\n",
       " (u'set', 2),\n",
       " (u'often', 1),\n",
       " (u'ASSIGNMENT', 1),\n",
       " (u'testing', 2),\n",
       " (u'HW', 6),\n",
       " (u'experiments', 1),\n",
       " (u'located', 1),\n",
       " (u'frequency', 1),\n",
       " (u'Build', 1),\n",
       " (u'X', 1),\n",
       " (u'measure', 1),\n",
       " (u'progress', 1),\n",
       " (u'Save', 1),\n",
       " (u'NOTE:', 1),\n",
       " (u'linear', 4),\n",
       " (u\"MLLib's\", 1),\n",
       " (u'parsedData', 1),\n",
       " (u'following:', 1),\n",
       " (u'10.1.1', 1),\n",
       " (u'Evaluate', 1),\n",
       " (u'separate', 1),\n",
       " (u'find', 1),\n",
       " (u'iterations,', 2),\n",
       " (u'weighted', 1),\n",
       " (u'between', 2),\n",
       " (u'it', 3),\n",
       " (u'import', 3),\n",
       " (u'Regression', 1),\n",
       " (u'be', 2),\n",
       " (u'Short', 1),\n",
       " (u'creating', 1),\n",
       " (u'found', 2),\n",
       " (u'team', 1),\n",
       " (u'points', 2),\n",
       " (u'occurence.', 1),\n",
       " (u'str(WSSSE))', 1),\n",
       " (u'snippet:', 1),\n",
       " (u'Hadoop?', 1),\n",
       " (u'10.6.2', 1),\n",
       " (u'by', 1),\n",
       " (u'after', 3),\n",
       " (u'Justify', 1),\n",
       " (u'clusters.centers[clusters.predict(point)]', 1),\n",
       " (u'languages).', 1),\n",
       " (u'(OPTIONAL)', 2),\n",
       " (u'via', 2),\n",
       " (u'of', 17),\n",
       " (u'la', 1),\n",
       " (u'KMEans', 1),\n",
       " (u'example', 4),\n",
       " (u'massive', 1),\n",
       " (u'https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form',\n",
       "  1),\n",
       " (u'maxIterations=10,', 1),\n",
       " (u'or', 2),\n",
       " (u'first', 1),\n",
       " (u'computational', 1),\n",
       " (u'assignments', 1),\n",
       " (u'Team', 1),\n",
       " (u'findings.', 2),\n",
       " (u'one', 2),\n",
       " (u'Scale', 1),\n",
       " (u'computing', 1),\n",
       " (u'DropBox', 1),\n",
       " (u'center)]))', 1),\n",
       " (u'your', 7),\n",
       " (u'LinearRegressionWithSGD', 1),\n",
       " (u'are', 1),\n",
       " (u'plot', 3),\n",
       " (u'management', 1),\n",
       " (u'plots', 1),\n",
       " (u'give', 1),\n",
       " (u'appears', 1),\n",
       " (u'INSTURCTIONS', 1),\n",
       " (u'2', 1),\n",
       " (u'parsedData.map(lambda', 1),\n",
       " (u'Plot', 1),\n",
       " (u'thru', 1),\n",
       " (u'more', 1),\n",
       " (u'Explain.', 2),\n",
       " (u'provide,', 1),\n",
       " (u'on', 7),\n",
       " (u'iteration,', 1),\n",
       " (u'differences', 2),\n",
       " (u'||X||', 1),\n",
       " (u'back', 1),\n",
       " (u'RDD', 1),\n",
       " (u'norm):', 1),\n",
       " (u'10.1:', 1),\n",
       " (u'===MIDS', 1),\n",
       " (u'clusters', 4),\n",
       " (u'(and', 1),\n",
       " (u'10', 1),\n",
       " (u'Set', 4),\n",
       " (u'clusters.save(sc,', 1),\n",
       " (u'\"myModelPath\")', 2),\n",
       " (u'up', 2),\n",
       " (u'Generate', 2),\n",
       " (u'cluster', 1),\n",
       " (u'HW10.3', 2),\n",
       " (u'can', 1),\n",
       " (u'iterations.', 1),\n",
       " (u'KMeansModel', 1),\n",
       " (u'Your', 1),\n",
       " (u'(list', 1),\n",
       " (u'KMeans', 6),\n",
       " (u'vector', 2),\n",
       " (u'exercise.', 1),\n",
       " (u'evaluation.', 1),\n",
       " (u'X1', 1),\n",
       " (u'print(\"Within', 1),\n",
       " (u'MLLib', 1),\n",
       " (u'an', 3),\n",
       " (u'words.', 1),\n",
       " (u'at', 1),\n",
       " (u'in', 18),\n",
       " (u'experiements', 1),\n",
       " (u'work', 1),\n",
       " (u'any', 2),\n",
       " (u'Then', 1),\n",
       " (u'data.', 1),\n",
       " (u'line:', 1),\n",
       " (u'Java,', 1),\n",
       " (u'that', 2),\n",
       " (u'regression', 3),\n",
       " (u'instance', 1),\n",
       " (u'other', 1),\n",
       " (u'\"', 1),\n",
       " (u'document', 3),\n",
       " (u'(regularization)\".', 1),\n",
       " (u'blanks', 1),\n",
       " (u'provided', 3),\n",
       " (u'assignment', 1),\n",
       " (u'DATA:', 1),\n",
       " (u'10.3:', 1),\n",
       " (u'y:', 1),\n",
       " (u'applications', 1),\n",
       " (u'notebook', 1),\n",
       " (u'x', 3),\n",
       " (u'such', 1),\n",
       " (u'repeat', 2),\n",
       " (u'descent', 1),\n",
       " (u'center', 1),\n",
       " (u'sets', 1),\n",
       " (u'element', 1),\n",
       " (u'train', 1),\n",
       " (u'decreasing', 2),\n",
       " (u'code)', 1),\n",
       " (u'HW10.4.', 1),\n",
       " (u'order', 2),\n",
       " (u'Fill', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "data = sc.textFile('homework_10.txt')\n",
    "\n",
    "# Split and tokenize \n",
    "tokens = data.flatMap(lambda x: x.split()).map(lambda x: (x, 1))\n",
    "\n",
    "# Count \n",
    "wordCounts = tokens.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Return \n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Question 10.1.1\n",
    "\n",
    "*Modify the above word count code to count words that begin with lower case letters (a-z) and report your findings. Again sort the output words in decreasing order of frequency.*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "We use similar code as above, but include an additional map phase to remove tokens that do not begin with lower-case letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'y)', 1),\n",
       " (u'sameModel', 1),\n",
       " (u'all', 1),\n",
       " (u'homeworks', 1),\n",
       " (u'intuitoive', 1),\n",
       " (u'consists', 1),\n",
       " (u'snippet', 1),\n",
       " (u'based', 1),\n",
       " (u'parameters', 1),\n",
       " (u'error(point):', 1),\n",
       " (u'had', 1),\n",
       " (u'languages', 1),\n",
       " (u'weeks', 1),\n",
       " (u'submissions', 1),\n",
       " (u'homework', 2),\n",
       " (u'count', 3),\n",
       " (u'return', 1),\n",
       " (u'runs=10,', 1),\n",
       " (u'load', 1),\n",
       " (u'runs', 1),\n",
       " (u'experiment.', 1),\n",
       " (u'words', 3),\n",
       " (u'report', 3),\n",
       " (u'compute', 1),\n",
       " (u'sc.textFile(\"kmeans_data.txt\")', 1),\n",
       " (u'bringing', 1),\n",
       " (u'point:', 1),\n",
       " (u'notebook:', 2),\n",
       " (u'resource', 1),\n",
       " (u'questions===', 1),\n",
       " (u'array([float(x)', 1),\n",
       " (u'evaluation', 2),\n",
       " (u'frequency.', 1),\n",
       " (u'list', 2),\n",
       " (u'where', 1),\n",
       " (u'initializationMode=\"random\")', 1),\n",
       " (u'generation', 1),\n",
       " (u'provided).', 1),\n",
       " (u'run', 2),\n",
       " (u'regression.', 2),\n",
       " (u'sqrt(sum([x**2', 1),\n",
       " (u'for', 11),\n",
       " (u'please', 1),\n",
       " (u'per', 2),\n",
       " (u'cell', 1),\n",
       " (u'blanks:', 1),\n",
       " (u'above', 1),\n",
       " (u'at:', 1),\n",
       " (u'math', 1),\n",
       " (u'available', 3),\n",
       " (u'lazy', 3),\n",
       " (u'https://www.dropbox.com/s/q85t0ytb9apggnh/kmeans_data.txt?dl=0', 2),\n",
       " (u'interfaces', 1),\n",
       " (u'modify', 1),\n",
       " (u'here', 2),\n",
       " (u'following', 3),\n",
       " (u'not', 1),\n",
       " (u'group', 1),\n",
       " (u'training', 3),\n",
       " (u'column', 1),\n",
       " (u'completing', 1),\n",
       " (u'implementation', 1),\n",
       " (u'length', 1),\n",
       " (u'resulting', 1),\n",
       " (u'follows:', 1),\n",
       " (u'pyspark.mllib.clustering', 1),\n",
       " (u'comment', 1),\n",
       " (u'letters', 1),\n",
       " (u'distributed', 1),\n",
       " (u'done', 1),\n",
       " (u'iterations', 2),\n",
       " (u'array', 1),\n",
       " (u'https://www.dropbox.com/s/3nsthvp8g2rrrdh/EM-Kmeans.ipynb?dl=0', 1),\n",
       " (u'clustering', 1),\n",
       " (u'use', 1),\n",
       " (u'from', 5),\n",
       " (u'findings', 1),\n",
       " (u'submit', 1),\n",
       " (u'error(point)).reduce(lambda', 1),\n",
       " (u'forward', 1),\n",
       " (u'numpy', 1),\n",
       " (u'sort', 1),\n",
       " (u'a', 12),\n",
       " (u'form', 1),\n",
       " (u'manner.', 1),\n",
       " (u'with', 7),\n",
       " (u'case', 1),\n",
       " (u'word', 2),\n",
       " (u'this', 7),\n",
       " (u'single', 1),\n",
       " (u'labeled', 1),\n",
       " (u'fun', 1),\n",
       " (u'def', 1),\n",
       " (u'and', 23),\n",
       " (u'model?', 1),\n",
       " (u'links', 1),\n",
       " (u'is', 8),\n",
       " (u'parse', 1),\n",
       " (u'as', 5),\n",
       " (u'tab', 1),\n",
       " (u'different', 1),\n",
       " (u'develop', 1),\n",
       " (u'x,', 1),\n",
       " (u'provide', 1),\n",
       " (u'set.', 2),\n",
       " (u'weight(X)=', 1),\n",
       " (u'sqrt', 1),\n",
       " (u'write', 1),\n",
       " (u'answer', 1),\n",
       " (u'clusters.', 2),\n",
       " (u'program.', 1),\n",
       " (u'plots.', 2),\n",
       " (u'begin', 1),\n",
       " (u'https://docs.google.com/spreadsheets/d/1ncFQl5Tovn-16slD8mYjP_nzMTPSfiGeLLzW8v_sMjg/edit?usp=sharing',\n",
       "  1),\n",
       " (u'driver', 1),\n",
       " (u'data)', 1),\n",
       " (u'data.map(lambda', 1),\n",
       " (u'data', 8),\n",
       " (u'the', 46),\n",
       " (u'tune', 1),\n",
       " (u'documents).', 1),\n",
       " (u'tokens', 1),\n",
       " (u'lower', 1),\n",
       " (u'savings', 1),\n",
       " (u'algorithms', 1),\n",
       " (u'model', 3),\n",
       " (u'hyper', 1),\n",
       " (u'code', 9),\n",
       " (u'show', 1),\n",
       " (u'text', 1),\n",
       " (u'results', 3),\n",
       " (u'experiments', 1),\n",
       " (u'using', 3),\n",
       " (u'follow', 1),\n",
       " (u'progress', 1),\n",
       " (u'find', 1),\n",
       " (u\"line.split('\", 1),\n",
       " (u'inverse', 1),\n",
       " (u'instance', 1),\n",
       " (u'to', 9),\n",
       " (u'going', 1),\n",
       " (u'team)', 1),\n",
       " (u'clusters', 4),\n",
       " (u'do', 1),\n",
       " (u'good', 1),\n",
       " (u'get', 1),\n",
       " (u'evaluate', 1),\n",
       " (u'findings.', 2),\n",
       " (u'framework', 1),\n",
       " (u'carefully.', 1),\n",
       " (u'sorted', 1),\n",
       " (u'dataset', 1),\n",
       " (u'instructions', 1),\n",
       " (u'evaluation.', 1),\n",
       " (u'follows', 1),\n",
       " (u'https://www.dropbox.com/s/atzqkc0p1eajuz6/LinearRegression-Notebook-Challenge.ipynb?dl=0',\n",
       "  1),\n",
       " (u'server', 1),\n",
       " (u'assignments', 1),\n",
       " (u'y:', 1),\n",
       " (u'either', 1),\n",
       " (u'each', 4),\n",
       " (u'x', 3),\n",
       " (u'set', 2),\n",
       " (u'often', 1),\n",
       " (u'parsedData.map(lambda', 1),\n",
       " (u'testing', 2),\n",
       " (u'back', 1),\n",
       " (u'clusters.save(sc,', 1),\n",
       " (u'frequency', 1),\n",
       " (u'are', 1),\n",
       " (u'measure', 1),\n",
       " (u'experiements', 1),\n",
       " (u'linear', 4),\n",
       " (u'parsedData', 1),\n",
       " (u'following:', 1),\n",
       " (u'iterations.', 1),\n",
       " (u'print(\"Within', 1),\n",
       " (u'separate', 1),\n",
       " (u'iterations,', 2),\n",
       " (u'between', 2),\n",
       " (u'import', 3),\n",
       " (u'be', 2),\n",
       " (u'creating', 1),\n",
       " (u'found', 2),\n",
       " (u'team', 1),\n",
       " (u'how', 2),\n",
       " (u'occurence.', 1),\n",
       " (u'str(WSSSE))', 1),\n",
       " (u'snippet:', 1),\n",
       " (u'plots', 1),\n",
       " (u'by', 1),\n",
       " (u'on', 7),\n",
       " (u'languages).', 1),\n",
       " (u'via', 2),\n",
       " (u'of', 17),\n",
       " (u'la', 1),\n",
       " (u'massive', 1),\n",
       " (u'https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form',\n",
       "  1),\n",
       " (u'maxIterations=10,', 1),\n",
       " (u'or', 2),\n",
       " (u'first', 1),\n",
       " (u'computational', 1),\n",
       " (u'number', 1),\n",
       " (u'one', 2),\n",
       " (u'weighted', 1),\n",
       " (u'center)]))', 1),\n",
       " (u'your', 7),\n",
       " (u'plot', 3),\n",
       " (u'management', 1),\n",
       " (u'computing', 1),\n",
       " (u'appears', 1),\n",
       " (u'thru', 1),\n",
       " (u'more', 1),\n",
       " (u'provide,', 1),\n",
       " (u'that', 2),\n",
       " (u'iteration,', 1),\n",
       " (u'differences', 2),\n",
       " (u'train', 1),\n",
       " (u'made', 1),\n",
       " (u'cluster', 1),\n",
       " (u'work', 1),\n",
       " (u'up', 2),\n",
       " (u'can', 1),\n",
       " (u'norm):', 1),\n",
       " (u'clusters.centers[clusters.predict(point)]', 1),\n",
       " (u'example', 4),\n",
       " (u'hundred)', 1),\n",
       " (u'vector', 2),\n",
       " (u'exercise.', 1),\n",
       " (u'give', 1),\n",
       " (u'it', 3),\n",
       " (u'an', 3),\n",
       " (u'words.', 1),\n",
       " (u'at', 1),\n",
       " (u'in', 18),\n",
       " (u'any', 2),\n",
       " (u'data.', 1),\n",
       " (u'line:', 1),\n",
       " (u'output', 1),\n",
       " (u'kmeans_data.txt', 2),\n",
       " (u'regression', 3),\n",
       " (u'located', 1),\n",
       " (u'other', 1),\n",
       " (u'document', 3),\n",
       " (u'blanks', 1),\n",
       " (u'provided', 3),\n",
       " (u'assignment', 1),\n",
       " (u'after', 3),\n",
       " (u'applications', 1),\n",
       " (u'notebook', 1),\n",
       " (u'such', 1),\n",
       " (u'repeat', 2),\n",
       " (u'descent', 1),\n",
       " (u'center', 1),\n",
       " (u'sets', 1),\n",
       " (u'element', 1),\n",
       " (u'points', 2),\n",
       " (u'decreasing', 2),\n",
       " (u'code)', 1),\n",
       " (u'order', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# Filter \n",
    "lower = tokens.filter(lambda x: x[0][0] in letters)\n",
    "\n",
    "# Count \n",
    "wordCounts = lower.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Return \n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Question 10.2\n",
    "\n",
    "*Run the following code snippet and list the clusters that your find and compute the Within Set Sum of Squared Errors for the found clusters. Comment on your findings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 0.692820323028\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data\n",
    "# NOTE  kmeans_data.txt is available here \n",
    "#          https://www.dropbox.com/s/q85t0ytb9apggnh/kmeans_data.txt?dl=0 \n",
    "data = sc.textFile(\"kmeans_data.txt\")  \n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"myModelPath\")\n",
    "sameModel = KMeansModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.3\n",
    "\n",
    "*Download the following KMeans notebook:*\n",
    "\n",
    "https://www.dropbox.com/s/3nsthvp8g2rrrdh/EM-Kmeans.ipynb?dl=0\n",
    "\n",
    "*Generate 3 clusters with 100 (one hundred) data points per cluster (using the code provided). Plot the data.\n",
    "Then run MLlib's Kmean implementation on this data  and report your results as follows:*\n",
    "\n",
    "  -- *plot the resulting clusters after 1 iteration, 10 iterations, after 20 iterations, after 100 iterations.*\n",
    "  -- *in each plot please report the Within Set Sum of Squared Errors for the found clusters. Comment on the progress of this measure as the KMEans algorithms runs for more iterations*\n",
    "  \n",
    "#### Solution \n",
    "\n",
    "First, we must generate our points to be clustered. Below is the code used to accomplish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle \n",
    "\n",
    "\n",
    "# Generate points \n",
    "x = np.random.normal(loc=-5, scale=1, size=(100, 2))\n",
    "y = np.random.normal(loc=0, scale=1, size=(100, 2))\n",
    "z = np.random.normal(loc=5, scale=1, size=(100, 2))\n",
    "\n",
    "\n",
    "# Write to file \n",
    "with open('comparison.txt', 'w') as f: \n",
    "\n",
    "    for point in x: \n",
    "        f.write('%s,%s\\n' % (point[0], point[1]))\n",
    "        \n",
    "    for point in y: \n",
    "        f.write('%s,%s\\n' % (point[0], point[1]))\n",
    "        \n",
    "    for point in z: \n",
    "        f.write('%s,%s\\n' % (point[0], point[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated our points, we need to compute KMeans over the data. We will use our custom implementation to accomplish this. This includes classes to assign the clusters, update the clusters based on new assignment, and compute errors. Finally, we have a driver that wraps this processes together. \n",
    "\n",
    "##### Assigning Clusters \n",
    "\n",
    "Here, we assign each point a cluster based on which one it is closest to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing assign_clusters.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assign_clusters.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class assignClusters(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(assignClusters, self).configure_options()\n",
    "\t\tself.add_file_option('--centroids', \n",
    "\t\t\thelp='pointer to centroids file. See main runner for details.')\n",
    "\n",
    "\tdef load_options(self, args):\n",
    "\n",
    "\t\t\"\"\" This function initializes arguments defined in 'configure_options'. \"\"\"\n",
    "\n",
    "\t\tsuper(assignClusters, self).load_options(args)\n",
    "\t\t\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.centroids, 'r') as f: \n",
    "\t\t\tself.clusters = pickle.load(f)\n",
    "\n",
    "\n",
    "\tdef mapper_diff_comp(self, _, line):\n",
    "\n",
    "\t\tbody = [float(x) for x in line.split(',')]\n",
    "\t\tcustID = hash(''.join([str(x) for x in body]))\n",
    "\n",
    "\t\t# Compute distances \n",
    "\t\tfor clusterID, cluster in self.clusters.iteritems(): \n",
    "\t\t\t\n",
    "\t\t\tcluster = np.array(cluster)\n",
    "\t\t\tbody = np.array(body)\n",
    "\t\t\tweight = np.linalg.norm(body)\n",
    "\t\t\tdist = np.linalg.norm(body-cluster) / weight\n",
    "\n",
    "\t\t\tyield custID, [clusterID, dist]\n",
    "\n",
    "\n",
    "\tdef reducer_find_min_cluster(self, custID, distArray):\n",
    "\n",
    "\t\t# Find closest cluster\n",
    "\t\tdistArray = np.array(list(distArray))\n",
    "\t\tclusterIndex = np.argmin(distArray[:, 1])\n",
    "\t\tclusterID = distArray[clusterIndex, 0]\n",
    "\n",
    "\t\tyield custID, clusterID\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper_diff_comp, \n",
    "\t\t\t\t\t\treducer=self.reducer_find_min_cluster)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tassignClusters.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Clusters \n",
    "\n",
    "Once the clusters are assigned, they centers need to be updated so a new phase of iteration can begin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting update_centroids.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile update_centroids.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class updateCentroids(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(updateCentroids, self).configure_options()\n",
    "\n",
    "\t\tself.add_file_option('--scores', \n",
    "\t\t\thelp='pointer to scores file. See main runner for details.')\n",
    "\n",
    "\t\tself.add_file_option('--centroids', \n",
    "\t\t\thelp='pointer to centroids file. See main runner for details.')\n",
    "\n",
    "\t\tself.add_passthrough_option('--errors', \n",
    "\t\t\tdefault=False, help='If True, computes RMSE instead of updating the centroid centers.')\n",
    "\n",
    "\n",
    "\tdef load_options(self, args):\n",
    "\n",
    "\t\t\"\"\" This function initializes arguments defined in 'configure_options'. \"\"\"\n",
    "\n",
    "\t\tsuper(updateCentroids, self).load_options(args)\n",
    "\n",
    "\t\tself.errors = self.options.errors\n",
    "\n",
    "\t\t\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.scores, 'r') as f: \n",
    "\t\t\tself.scores = pickle.load(f)\n",
    "\n",
    "\n",
    "\t\t# Load centroids\n",
    "\t\twith open(self.options.centroids, 'r') as f: \n",
    "\t\t\tself.centroids = pickle.load(f)\n",
    "\n",
    "\n",
    "\tdef mapper_telegraph_id(self, _, line):\n",
    "\n",
    "\t\tbody = [float(x) for x in line.split(',')]\n",
    "\t\tcustID = hash(''.join([str(x) for x in body]))\n",
    "\n",
    "\t\t# Get individual\n",
    "\t\tlabel = self.scores[custID]\n",
    "\t\tyield label, body\n",
    "\n",
    "\n",
    "\tdef reducer_emit_clusters(self, label, arrays):\n",
    "\t\t\n",
    "\t\t# Compute new cluster \n",
    "\t\tarrays = list(arrays)\n",
    "\t\tnewCluster = [np.mean(x) for x in zip(*arrays)]\n",
    "\n",
    "\t\tyield label, newCluster\n",
    "\n",
    "\n",
    "\tdef reducer_compute_dist(self, label, vectors):\n",
    "\t\tvectors = np.array(list(vectors))\n",
    "\n",
    "\t\tcluster = self.centroids[label]\n",
    "\t\tdist = sum([np.linalg.norm(body-cluster) for body in vectors])\n",
    "\n",
    "\t\tyield None, dist\n",
    "\n",
    "\n",
    "\tdef reducer_rmse(self, _, dists):\n",
    "\n",
    "\t\t# Compute\n",
    "\t\tN = len(self.scores)\n",
    "\t\tRMSE = np.sqrt(sum(dists) / N)\n",
    "\n",
    "\t\tyield None, RMSE\t\t\t\n",
    "\n",
    "\n",
    "\tdef steps(self):\n",
    "\n",
    "\t\t\"\"\" Checks for the error flag to compute RMSE.\"\"\"\n",
    "\n",
    "\t\tif self.errors: \n",
    "\n",
    "\t\t\treturn [MRStep(mapper=self.mapper_telegraph_id, \n",
    "\t\t\t\t\t\t\treducer=self.reducer_compute_dist), \n",
    "\n",
    "\t\t\t\t\tMRStep(reducer=self.reducer_rmse)]\n",
    "\n",
    "\t\telse:\n",
    "\n",
    "\t\t\treturn [MRStep(mapper=self.mapper_telegraph_id,\n",
    "\t\t\t\t\t\t\treducer=self.reducer_emit_clusters)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tupdateCentroids.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Driver \n",
    "\n",
    "The driver file will manage the iteration process and prints the points as we iterations progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeans_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kmeans_driver.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "from assign_clusters import assignClusters\n",
    "from update_centroids import updateCentroids\n",
    "\n",
    "import cPickle as pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Storage files \n",
    "trackerFile = './.tracker'\n",
    "centroidFile = './.clusters'\n",
    "scoreFile = './.scores'\n",
    "dataFile = 'comparison.txt'\n",
    "\n",
    "\n",
    "def getName(obj, namespace):\n",
    "\treturn [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\n",
    "def extractValues(job, runner):\n",
    "\toutput = defaultdict(int)\n",
    "\tfor line in runner.stream_output(): \n",
    "\t\tkey, value = job.parse_output_line(line)\n",
    "\t\toutput[key] = value\n",
    "\n",
    "\treturn output \n",
    "\n",
    "\n",
    "def dumpToFile(variable, filename):\n",
    "\twith open(filename, 'w') as f: \n",
    "\t\tpickle.dump(variable, f)\n",
    "\n",
    "\n",
    "def dumpToTracker(variable, filename):\n",
    "\twith open(filename, 'a') as f: \n",
    "\t\tf.write('dumping...' + '\\n')\n",
    "\t\tf.write(str(variable) + '\\n')\n",
    "\t\tf.write('dump complete.' + '\\n')\n",
    "\n",
    "\n",
    "def runJob(method, args, dFile=centroidFile):\n",
    "\tjob = method(args=args)\n",
    "\n",
    "\tmethodName = getName(method, globals())[0]\n",
    "\tprint '\\n\\t' + 'Running ' + methodName + '...'\n",
    "\n",
    "\twith job.make_runner() as runner: \n",
    "\n",
    "\t\t# Surpress console \n",
    "\t\trunner.run()\n",
    "\n",
    "\t\tresult = extractValues(job, runner)\n",
    "\t\tdumpToTracker(result, trackerFile)\n",
    "\n",
    "\t\tprint '\\t' + 'Complete: ' + methodName\n",
    "\n",
    "\t\tif dFile: \t\t\t\n",
    "\t\t\tdumpToFile(result, dFile)\t\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t# Clear files \n",
    "\topen(trackerFile, 'w').close()\n",
    "\topen(centroidFile, 'w').close()\n",
    "\topen(scoreFile, 'w').close()\n",
    "\n",
    "\t# Step 1: Create initial clusters \n",
    "\twith open(centroidFile, 'w') as f: \n",
    "\t\t\n",
    "\t\tcentroidDict = {\n",
    "\t\t\t\n",
    "\t\t\t'A': [0, 0]\n",
    "\t\t\t, 'B' : [6, 3]\n",
    "\t\t\t, 'C' : [3, 6]\n",
    "\n",
    "\t\t}\n",
    "\n",
    "\t\tpickle.dump(centroidDict, f)\n",
    "\t\t\n",
    "\n",
    "\tmaxIter = 10\n",
    "\n",
    "\t# Loop\n",
    "\tfor i in range(maxIter):\n",
    "\n",
    "\t\tprint '\\n' + 'Iteration ' + str(i) + '.'\n",
    "\n",
    "\t\t# Score based on clusters \n",
    "\t\tcentroidArg = '--centroids='+centroidFile\n",
    "\t\trunJob(assignClusters, args=[dataFile, centroidArg], dFile=scoreFile)\n",
    "\n",
    "\t\t# Update clusters \n",
    "\t\tscoreArg = '--scores='+scoreFile\n",
    "\t\trunJob(updateCentroids, args=[dataFile, centroidArg, scoreArg])\n",
    "\n",
    "\t\t# Report errors \n",
    "\t\terror = runJob(updateCentroids, args=[dataFile, centroidArg, scoreArg, '--errors=True'], dFile=None)\n",
    "\t\terror = error.values()[0]\n",
    "\n",
    "\t\tprint '\\nRMSE: %s' % (round(error, 5))\n",
    "\n",
    "# \t\t# Load \n",
    "# \t\tdata = []\n",
    "# \t\twith open(dataFile, 'r') as f: \n",
    "# \t\t\tfor line in f.readlines(): \n",
    "\n",
    "# \t\t\t\tline = line.strip('\\n').split(',')\n",
    "# \t\t\t\tdata.append(line)\n",
    "\n",
    "# \t\t# Convert \n",
    "# \t\tdata = np.array(data)\n",
    "\t\t\n",
    "# \t\twith open(centroidFile, 'r') as f: \n",
    "# \t\t\tclusters = pickle.load(f)\n",
    "\n",
    "# \t\tclusters = np.array(clusters.values()) \n",
    "\n",
    "\n",
    "# \t\t# Plot \n",
    "# \t\tplt.plot(data[:, 0], data[:, 1], 'bo', clusters[:, 0], clusters[:, 1], 'ro')\n",
    "# \t\tplt.show()\n",
    "\t\t\n",
    "\n",
    "\t# Print clusters\n",
    "\twith open(centroidFile, 'r') as f: \n",
    "\t\tcenters = pickle.load(f)\n",
    "\n",
    "\t\tfor k, v in centers.iteritems():\n",
    "\t\t\tprint '\\n' + 'Cluster: %s' % k\n",
    "\t\t\tprint '\\t' + str(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute \n",
    "\n",
    "We run the code for KMeans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.68977\n",
      "\n",
      "Iteration 1.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.7355\n",
      "\n",
      "Iteration 2.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.72347\n",
      "\n",
      "Iteration 3.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.68313\n",
      "\n",
      "Iteration 4.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.37788\n",
      "\n",
      "Iteration 5.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.14293\n",
      "\n",
      "Iteration 6.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.12086\n",
      "\n",
      "Iteration 7.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.12086\n",
      "\n",
      "Iteration 8.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.12086\n",
      "\n",
      "Iteration 9.\n",
      "\n",
      "\tRunning assignClusters...\n",
      "\tComplete: assignClusters\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "\tRunning updateCentroids...\n",
      "\tComplete: updateCentroids\n",
      "\n",
      "RMSE: 1.12086\n",
      "\n",
      "Cluster: A\n",
      "\t[-5.1603890626994, -5.046010212082799]\n",
      "\n",
      "Cluster: C\n",
      "\t[4.727851239905199, 4.9071413885276005]\n",
      "\n",
      "Cluster: B\n",
      "\t[0.005319926326784002, 0.0700072158488175]\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_driver.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The progression of the RMSE is as one would expect. After about 8 Iterations, the RMSE remains unchanged, which suggests a local minimum has been reached. At this point, the cluster centers will remain unchanged for the remaining iterations. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.4 \n",
    "\n",
    "*Using the KMeans code (homegrown code) provided repeat the experiments in HW10.3. Comment on any differences between the results in HW10.3 and HW10.4. Explain.*\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Using the Spark implementation of KMeans, we run the algorithm easily: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 1.25632826978\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data\n",
    "# NOTE  kmeans_data.txt is available here \n",
    "#          https://www.dropbox.com/s/q85t0ytb9apggnh/kmeans_data.txt?dl=0 \n",
    "data = sc.textFile(\"comparison.txt\")  \n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.strip('\\n').split(',')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 3, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE / 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two striking differences between the Spark methodology and the MRJob implemetation. The first is that Spark finishes faster, even without using Hadoop. The second is that the MRJob implementation has slightly lower RMSE, but I suspect that is due to the convergence criteria being met. In the MRJob implementation, there is no stopping criteria. We see that an RMSE of `1.25` is reached somewhere between Iteration 4 and 5 for MRJob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
